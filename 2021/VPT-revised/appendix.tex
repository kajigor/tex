\newpage

\section*{Appendix}

We thank the reviewers for their thoughtful feedback.

\subsection*{Review 1}

The paper considers the optimisation of miniKanren relational programs using some form of (conjunctive) partial evaluation. In particular, the authors propose a novel approach, called conservative partial deduction, and reports on an experimental evaluation that compares the results obtained with ECCE (an online partial evaluator for Prolog based on conjunctive partial deduction) and the authors' tool. The results are very promising and show the advantages of the new approach.

All in all, the paper contains interesting ideas and I recommend it to be accepted for VPT 2021.

\emph{Thank you for the kind review.}

Detailed comments for authors:

\begin{itemize}
  \item In order for the paper to be as self-complete as possible, I'd suggest to add a brief introduction to miniKanren (syntax, informal semantics, a couple of examples) at the beginning of the paper.

  \emph{Added. See subsection \ref{mkIntro}.}

  \item There are already some works that considered non-leftmost unfolding during partial evaluation, e.g.,

  \begin{itemize}
    \item  E. Albert, G. Puebla, J.P. Gallagher: Non-leftmost Unfolding in Partial Evaluation of Logic Programs with Impure Predicates. LOPSTR 2005: 115-132
    \item M. Leuschel, G. Vidal: Fast offline partial evaluation of logic programs. Inf. Comput. 235: 70-97 (2014)
  \end{itemize}

  \emph{Thank you for bringing these to our attention. }
  \todo{Check: non-leftmost unfolding of what? What's the motivation?}

  \item As for the effectiveness of partial evaluation, you can check this one

  \begin{itemize}
    \item G. Vidal: Cost-Augmented Partial Evaluation of Functional Logic Programs. High. Order Symb. Comput. 17(1-2): 7-46 (2004)
  \end{itemize}

  where the effectiveness of partial evaluation is estimated, and this one

  \begin{itemize}
    \item G. Vidal: Trace Analysis for Predicting the Effectiveness of Partial Evaluation. ICLP 2008: 790-794
  \end{itemize}

  which aims at predicting the effectiveness achieved by partial evaluation
  (though it's just a preliminary approach).

  \emph{Thank you! These papers do provide a nice way to estimate how efficient a transformation is and we will likely employ these ideas in future versions of our specializer, although this would likely require some tweaking for the interleaving semantics of \mk. }

  \item The formalisation of conservative partial deduction using pseudocode is nice, but a more formal approach (as in [3]) might be useful to prove a number of properties. You could consider that as future work.

  \emph{We agree that our approach will benefit from a more formal description and will consider it as future work.}

  \item{In Sect. 3.1, you mention that unfolding too much might introduce extra unifications or duplicate computations. I think that this is not possible as long as partial evaluation considers a fixed left-to-right unfolding strategy at PE time. This is actually a problem when considering non-leftmost unfolding strategies at PE time.}

  \emph{Section 5.1.1 of the paper~\cite{de1999conjunctive} states that non-determinate unfolding may add unifications regardless of wether the unfolding strategy is left-to-right or not. Of course, in pure \pro the effect of using unfolding strategies which are not left-to-right is more pronounced. Nevertheless, interleaving semantics of \mk complicates everything further.}

  \item Another unfolding strategy that might be related to your heuristics
  is presented in

  \begin{itemize}
    \item G. Vidal: A Hybrid Approach to Conjunctive Partial Evaluation of Logic
    Programs. LOPSTR 2010: 200-214
  \end{itemize}

  where the notion of ``strongly regular logic programs'' is introduced in order to characterise the predicates whose unfolding cannot produce infinitely growing conjunctions at PE time.

  \todo{Check: It seems that there are not that many evaluators which are strongly regular logic programs.}

  \item {The experimental evaluation is certainly promising. Nevertheless,
  it would be great if you could put the implemented tool publicly available,
  including the source code of the considered benchmarks so that the readers
  can replicate the experiments.}

  \emph{The tool is available on github: \url{https://github.com/kajigor/uKanren_transformations/}, the experiments, currently not very well structured, are also on github in a different repository: \url{https://github.com/kajigor/mk-transformers-bench}}
\end{itemize}



\subsection*{Review 2}

This paper looks at partial deduction for the relational programming language  MINIKANREN.
The execution of programs in MINIKANREN differs from that in Prolog in that the subgoals
of a conjunction or disjunction can be reordered. This creates many more degrees of freedom
in the way a program can be executed, and also in the optimisations that can be performed.

The paper looks at issues faced by conjunctive partial deduction of MINIKANREN and describes
a new approach to partial deduction for relational languages called conservative partial
deduction. The main issue with conjunctive partial deduction of MINIKANREN is the unfolding
strategy. For Prolog, this is usally done by deterministic unfolding where all atoms except
one are only unfolded if it matches with at most one clause head. One atom, usually the
leftmost one, can be unfolded non-deterministically. However, this can add many new relation
calls to a conjunction. Although this works well for Prolog, it does not work so well for
MINIKANREN as it does not match its order of evaluation and can result in large less efficient
programs.

The solution proposed to this problem in this paper is called conservative partial deduction,
which is inspired by both partial deduction and supercompilation. One aim is to create a
speialization algorithm that is simpler than conjunctive partial deduction, but I am not sure
this has been achieved as it still seems quite complex. The conservative aspect of the algorithm
is in the unfolding of relation calls. This involves deciding if unfolding a relation call can
lead to discovery of contradictions between conjuncts which in turn leads to restriction of the
answer set at specialization-time. It is unclear how exactly this is preferrable to unfolding any
other relation call. It might actually add many new relation calls to a conjunction and result in
large less efficient programs. The result of the unfolding is joined back into the conjunction
rather than being split as is done in conjunctive partial deduction. It also just stops on
encountering an embedding rather than trying to generalise and further transform. The algorithm
does not take advantage of the fact that subgoals in the language can be reordered, so it appears
to miss out on many further opportunities for optimisation.

The evaluation of the proposed technique is rather poor, with only variations of two test programs
being evaluated. For the evaluator of logic formulas, the variations are on the way the boolean
connectives are implemented, and whether these connectives are placed before or after the recursive call.
For the type checker, the variations are on whether it was implemented by hand or automatically generated
from a corresponding functional program. This is not enough to conclude whether the transformation
works well in practice. The transformed programs do all perform better than the original program,
but ECCE does perform better than the described transformation for one program variation. The order
of the answers produced by transformed programs can also be different to that of the original program,
which I see as very problematic as the transformation is changing the behaviour of the program.

Although the paper is well written, the contributions it makes to the area are minimal.
It does not seem appropriate to apply conjunctive partial deduction to transform MINIKANREN
programs, as conjunctive partial deduction follows the evaluation strategy of logic languages
such as Prolog rather than that of relational languages such as MINIKANREN. It would seem to
be more appropriate to devise a new transformation that follows the search strategy used by
the MINIKANREN implementation. For the correctness of the transformation, it should be proved
that the transformation does not change the order of answers produced for the program. A more
thorough evaluation of the described technique is also required. I am therefore on the fence
as to whether to accept this paper or not.

Detailed Comments
=================

Section 1, second last para: This opens a yet another possibility -> This opens yet another possibility

Section 2, para 2: is so-called process tree -> is a so-called process tree

Section 2, para 2: Of course, process tree -> Of course, the process tree

Section 2, para 2: signalls -> signals

Section 2, para 3: the efficiency of residual program -> the efficiency of a residual program

Section 2, para 3: yielding standalone compilation more powerful -> making standalone compilation more powerful

Section 2, final para: empitical -> empirical

Section 3, para 5: use a heuristics which decides ->  use a heuristic which decides

Section 3, para 5: if the heuristics fails to select -> if the heuristic fails to select

Section 3, final para: only if it is meaningful - what is meant by "meaningful"?

Section 3.1, para 2: Unlike functional and imperative languages, in logic and relational programming languages unfolding
may easily affect the target program’s efficiency - actually unfolding in functional and imperative languages can also
affect the program's efficiency if paramters are non-linear.

Section 3.2, Figure 2: this figure is not really necessary and does not add very much

Section 3.2, para 3: the less-branching heuristics -> the less-branching heuristic

Section 4, para 4: generated from the functional program - you have not really said what this functional program is

Section 4.1.1, para 2: direction data - it is not clear what is meant by this

Section 4.1.2, para 1: mimicking a truth tables -> mimicking a truth table

Section 4.2, para 3: in form of extra unifications -> in the form of extra unifications

Section 5, para 1: which uses a heuristics -> which uses a heuristic

Section 5, para 2: with regrads to -> with regards to

Section 5, para 3: were improved better -> were improved more

\subsection*{Review 3}

This paper seems to show that programs can be constructed that perform better using one algorithm (for MiniKanren) than another (ECCE for Prolog).  As mentioned on page 2, "there are no ways to predict the effect of specialization on a given program in general".  So it is hard to see what general conclusions can be drawn from this empirical study.

Many parts of the paper are written clearly but overall, more clarification of the purpose and significance of the experiment is needed.

This paper reminds me of some of the discussions about control of partial evaluation of logic programs in the 1990s.  It seems that some of these issues are being rediscovered in the context of MiniKanren, for example the handling of deterministic choices.  See for example Section 4 of [2].

MiniKanren and Prolog are similar languages, as shown by the experiments in which programs are translated from one language to the other.  However they have differences in their execution strategy and the translation could affect the execution.  This is a vital aspect that needs to be addressed.  Each language implementation is optimised for its respective execution strategy. However, all the experimental results seem to be achieved by running the programs as MiniKanren programs.  I.e. a Prolog program extracted from ECCE is executed as a MiniKanren program!


This seriously detracts from the value of the experiment. I think that the experiments should go both ways; i.e. run all the programs both as Prolog and as MiniKanren.



Detailed comments.

page 1. impelement ==> implement

page 2.  "What is worse, the efficiency of residual program from the target language evaluator point of view is rarely considered in the literature." This is certainly not true of PE for Prolog.  See for example references [1], [2].

page 3.  While it is true that CPD (ECCE) optimises programs for a left-to-right strategy. CPD can unfold any atom in a conjunction.  This preserves its logical semantics.  It is not true to state that CPD only "unfolds atoms from left to right".

page 3.  The strategy for CPD provided by ECCE can be varied, as mentioned.  However, the experiments apparently only use the default settings.

page 3. "The strategy of unfolding atoms from left to right is reasonable in the context of PROLOG because it mimics the way programs in PROLOG execute. But it often leads to larger global control trees and, as a result, bigger, less efficient programs."  This is misleading, possible false.  Unfolding the leftmost atoms can clearly NEVER increase the size of the search space when executing left to right.  It merely pushes forward some choices (which often can improve indexing in the head clause and improve performance). So what is meant by "larger global trees" and "less efficient programs". Please provide a clear example.

However, unfolding non-deterministic non-leftmost atoms CAN increase the search space of a left-right execution, since goals to the left of the unfolded atom are duplicated.


page 3. A feature of CPD that seems to be ignored is that a conjunction can be unfolded, i.e. it is not always "atom-by-atom" but a conjunction p,q can be treated as a single predicate pq.  This allows more powerful specialisations, loop fusion, etc.  It is not clear whether MiniKanren can achieve these specialisations.


page 4.  Note that putting formulas in canonical form (disjunction of conjunctions) blows up the size of the program and can also increase the search.  This is a critical part of the experiment that needs to be addressed.  It would be better to define a new predicate for each construct, and then interpret it.  E.g.

p :- s,(q \/ r).

should be translated as

p :- s,qr.

qr :- q.
qr :- r.

rather than

p :- s,q.
p :- s,r.

Executing the second form repeats execution of s, but the first form does not.  If the normalize function uses the second form, it casts serious doubt about the whole experiment.

page 5.  The mechanism of generalisation seems very crude.  It might work for the given examples but seems too simple to be used in general.  More powerful generalisation using abstract interpretation and related methods have been widely studied.

page 6.  Please explain more clearly why the two problems were chosen. It is stated that they are examples of relational interpreters to solve search problems. This is a special kind of program - can the results be expected to apply to other programs?

page 9, Fig 3.  It should be made absolutely clear whether the programs are all run in MiniKanren (not Prolog). In my opinion this casts doubt on the validity of the experiments, since ECCE is designed to optimised execution for Prolog.


[1] Raf Venken, Bart Demoen:
A Partial Evaluation System for Prolog: some Practical Considerations. New Gener. Comput. 6(2\&3): 279-290 (1988)

[2] J.P. Gallagher: Tutorial on Logic Program Specialisation"  (PEPM 1993).

\subsection*{Review 4}

The paper reports on testing the well-known ECCE partial deduction system, in order to compare with a partial deduction for a simplest model variant of Prolog named relation language system being developed by the authors.
It is surely in the scope of the VPT workshop.
The submitted paper is quite good smoothly written. It is rather a kind of a tutorial based on the simplest model relation language than a research paper. I have actually found no research news in this paper. The authors are studying the partial deduction method and try, with a youthful enthusiasm, to dubiously comment some aspects of the simplest version of ("positive") supercompilation, experimenting with simple program samples, and fix their understanding "on the white lists of paper". Nevertheless, studying the program specialization methods is laudable and should be supported.
The introduction, the section "Related works", and the section "References" take five pages from 13 pages used for the whole paper, i.e., these three sections together take more than 38 percents of the paper size.
Unfortunately, the paper contains no transformation example of miniKanren program given in details, thus the reader is not able to follows the details and be sure in soundness of the short comments.
The paper authors do not refer to many well known published papers that of course were read by them and indirectly used in the submitted tutorial. Since the authors do not explicitly formulate their contribution, then I think perhaps the authors understand themselves that the "conservative partial deduction" was studied in published literature and its variants were used in a number of partial deduction systems.
Thus I am forced to estimate this submission only with "a border paper" score.


Abstract:
1) We identify a number of issues, caused by MINIKANREN peculiarities, and describe a novel approach to specialization based on partial deduction and supercompilation.
 --- This reviewer is not sure the described approach is novel, but it is certainly useful for empirical study of partial deduction.
Sec. Introduction
Page 1.
2) --- In the first three paragraphs of this section the authors write their current understanding the basic concepts of the logical programming, assuming the readers are not familiar with the terms of  "relation language", "partial deduction", and "supercompilation". I think the whole account of these paragraphs can be written in a couple sentences.
3) Specialization or partial evaluation [7] is a technique aimed at improving the performance of a program given some information about it beforehand.
 --- Specialization is a wider concept, than the partial evaluation. The last one is just one of the program specialization methods.
4) Typo:
   (i.e. the length of an input list) --> (e.g. the length of an input list)
5) Control issues in partial deduction of logic programming language PROLOG have been studied before [13]. Unlike Prolog, where atoms in the right-hand side of a clause cannot be arbitrarily reordered without changing the semantics of a program, in MINIKANREN the subgoals of conjunction/disjunction can be freely switched. This opens a yet another possibility for optimization, not taken into account by approaches initially developed in the context of conventional logic programming.
 --- Actually, studying a program specialization in terms of any programming language starts (and goes on) only for a pure functional fragment of the language.

Page 2.
6) In this paper, we study issues which conjunctive partial deduction faces being applied for MINIKANREN. We also describe a novel approach to partial deduction for relational programming, conservative partial deduction. We implemented this approach and compared it with the existing specialization system (ECCE) for several programs. We report here the results of the comparison and discuss why some MINIKANREN programs run slower after specialization.
 --- It is the contribution? https://github.com/JetBrains-Research/OCanren

Sec. RelatedWork

Page 2.
7) Specialization is an attractive technique aimed to improve the performance of a program if some of its arguments are known statically.
 --- It is wrong. Actually the program specialization takes also into account contexts of intermediate computation, for example, composition of function (or relation) calls as your use themselves, trying to permutate calls of the commutative and associative basic logical connectives.
8) The heart of supercompilation-based techniques is driving - a symbolic execution of a program through all possible execution paths.
 --- As a rule, a program cannot be executed "through all possible execution paths", since the number of the paths are usually unbounded and lengths of the most of the paths are usually infinite. Your next sentence "The result of driving is so-called process tree where nodes correspond to configurations which represent computation state." contradicts to the previous one.
9) The two main sources for supercompilation optimizations are aggressive information propagation about variables' values, equalities and disequalities, and precomputing of all deterministic semantic evaluation steps.
 --- It seems to me you mean only deterministic programming languages. Hence, by definition, any "deterministic semantic evaluation step" is deterministic. It should be something like the following. "… precomputing all evaluation actions that can be uniformly done over unknown values of the program variables".
10) Generalization, abstracting away some computed data about the current term, makes folding possible.
 --- As a rule, a (local) generalization is a two-arity function depending on two configurations and, maybe, some nontrivial properties of the configuration automatically discovered by supercompilation before the corresponding generalization invocation starts.
11) The accumulating parameter can be removed by replacing the call with its generalization.
 --- As a rule, the accumulating parameter cannot be removed by generalization; even more the task to recognize the fact that a parameter accumulating is undecidable.
12) There are several ways to ensure process correctness and termination, most-specific generalization (anti-unification) and homeomorphic embedding [6, 10] as a whistle being the most common.
 --- In general, the most-specific generalization is not able prevent nontermination. Actually, it depends on the data set of the corresponding programming language. For example, if the data set is the set of finite strings in a given alphabet, then your statement is wrong. I.e. there are functional programming languages manipulating the character strings, rather than the LISP lists. In the case a programming language is Turing complete, using the most-specific generalization, as a rule, does not lead to any interesting transformations.
13) Other criteria, like the size of the generated program or possible optimizations and execution cost of different language constructions by the target language evaluator, are usually out of consideration [7].
 --- The authors do not understand that any (physical) computing system specified by a given program is unclosed. Hence, its performance efficiency depends on an external environment. The environment comprises, for example, (maybe) a number of sequential compilers transforming the source and residual programs to a code supported by hardware, and the hardware properties.

Page 3.
14) Unfortunately there is neither an automatic procedure to choose what control setting is likely to improve input programs the most nor any informal recommendations on how to choose the best settings.
 --- The sentence above should be rewritten.

Sec. 3 Conservative Partial Deduction
Page 4.
15) A driving process creates a process tree, from which a residual program is later created.
 --- The driving does not include the generalization and folding actions. Unfortunately, the authors being freshmen and a fresh woman do not understand the basic concepts of supercompilation.
16) --- In this section a transformation example demonstrating the "conservative partial deduction" algorithm should given in details. You have spent only 13 pages and have at least two pages for such an example.  There are many problems hidden behind the pseudocode given in Figure 1. Without such an example it is not certainly that the authors understand the problems. For example, what is the strategy looking for the configuration C'? (1) Does the configuration belong to the path rooted at the initial goal and ending at the configuration C'? (2) Or, maybe, does your algorithm look among all generated configurations in the process tree? What is happened when there are at least two such configurations? Following the pseudocode one may guesses that the strategy follows the (1)-st variant. Actually, it is not the best choice. As far as I know the most specialization methods based on the the (2)-nd variant. It is quite natural since the (1)-st strategy leads to generating lager !
 residual programs as compared with the (2)-nd one (or code duplication).
17) "heuristically_select_a_call( r_1 , … , r_n )
 --- Hence, you have no idea on the strategy choosing the next atom. Actually, the set of the strategies controlling the supercompilation process is the key problem hidden by you, since the optimization task per se is undecidable.
18) The substitution computed at each step is also stored in the tree node, although it is not included in the configuration.
 --- What does that mean? Only either an example mentioned above or a precise definition may provide information on the subject.
19) Each other disjunct takes the form of a possibly empty conjunction of relation calls accompanied with a substitution computed from unifications. Any MINIKANREN term can be trivially transformed into the described form. The function normalize in Fig. 1 is assumed to perform term normalization. The code is omitted for brevity.
 --- The code should be presented somewhere in the paper or in an appendix.
20) There are several core ideas behind this algorithm.
 --- Ok. See my remarks above.
21) The first is to select an arbitrary relation to unfold, not necessarily the leftmost which is safe. … heuristically_select_a_call stands for heuristics combination, see section 3.2 for details ...
 --- Let us see below.
22) The second idea is to use a heuristics which decides if unfolding a relation call can lead to discovery of contradictions between conjuncts which in turn leads to restriction of the answer set at specialization-time
 --- It is not a news. There is a huge literature devoted to this subject. For example, a large series of works by A. Pettorossi, M. Proietti and theirs coauthors, unfortunately, not cited by the authors of the paper being reviewed.
 --- Typo (here and many times along the papers): a heuristics --> a heuristic

Page 5.
23) In this approach, we do not generalize in the same fashion as CPD or supercompilation. Our conjunctions are always split into individual calls and are joined back together only if it is meaningful. If the need for generalization arises, i.e. homeomorphic embedding of conjunctions [3] is detected, then we immediately stop driving this conjunction (line 12). When residualizing such a conjunction, we just generate a conjunction of calls to the input program before specialization.
 --- In order to make convincing the decisions above you have to demonstrate them by means of interesting non-trivial examples.

Sec. 3.1 Unfolding
24) Unfolding too much may create extra unifications, which is by itself a costly operation, or even introduce duplicated computations by propagating the results of unfolding onto neighbouring conjuncts.
 --- Actually none can reason on any efficiency without fixing an efficiency model. As I have noted above the (physical) computing system is unclosed. The efficiency can be seen as kind of energy of the computing system. The task we are interested in is not quite mathematical and sometimes we should look for physical arguments.
25) We believe that the following heuristics provides a reasonable approach to unfolding control.
 --- I do not think that the statement above looks convincing. Do you?
 --- Only a meaningful list of interesting program transformation examples may convince that.


3.2 Less Branching Heuristics

Page 6.
4 Evaluation
26) ECCE is designed for PROLOG programming language and cannot be directly applied for programs, written in MINIKANREN. To be able to compare our approach with ECCE, we converted each input program first to the pure subset of PROLOG, then specialized it with ECCE, and then we converted the result back to MINIKANREN. The conversion to PROLOG is a simple syntactic conversion. In the conversion from PROLOG to MINIKANREN, for each Horn clause a conjunction is generated in which unifications are placed before any relation call.
 --- Actually ECCE can be (almost) directly applied for programs, written in MINIKANREN. You have to implement an interpreter of MINIKANREN in Prolog and specialize the interpreter w.r.t. your MINIKANREN programs. I expect you are not able to specialize a given Prolog interepreter, written in MINIKANREN,  w.r.t. any Prolog program. Are you able?

Page 7.
27) The queries were run on a laptop running Ubuntu 18.04 with quad core Intel Core i5 2.30GHz CPU and 8 GB of RAM. The tables and graphs use the following denotations. Original represents the execution time of a program before any transformations were applied; ECCE - of the program specialized by ECCE with default conjunctive control setting; ConsPD-of the program specialized by our approach. … Figure 3: Execution time of eval$^o$.
 --- I guess you have a series of examples demonstrating a different behaviour of ConsPD as compared with ECCE, but hide such examples. Do not have you?
 --- Have you explored some properties of the operating systems when the examples were specialized by both ConsPD and ECCE? For instance, how much of RAM was occupied, how many program were loaded in the operating system before starting and during the specialization?

Sec. 4.1 Evaluator of Logic Formulas
28) We specialize the eval$^o$ relation to synthesize formulas which evaluate to "true". To do so, we run the specializer for the goal with the last argument fixed to "true", while the first two arguments remain free variables. Depending on the way the eval$^o$ is implemented, different specializers generate significantly different residual programs.
 --- Those are interesting experiments. The specialization results are quite expected.

Sec. 4.1.1 The Order of Relation Calls

Page 8.
29) Knowing that res is "true", we can conclude that in the call and$^o$ v w res variables v and w have to be "true as well. There are three possible options for these variables in the call or$^o$ v w res and one for the call not$^o$. These variables are used in recursive calls of eval$^o$ and thus restrict the result of driving. CPD fails to recognize this, and thus unfolds recursive calls of eval$^o$ applied to fresh variables. It leads to over-unfolding, large residual programs and poor performance.
 --- The problem discussing by you above is a problem of parallel evaluation. Parallel evaluation is natural for the relation programming. It is widely known that the simplest way to solve the problem is to use the breadth-first unfolding.

Sec. 4.1.2 Unfolding of Complex Relations
30) Depending on the way a relation is implemented, it may take a different number of driving steps to reach the point when any useful information is derived through its unfolding.
 --- Here, for the first time in this paper, you use the "driving" term in a correct context, writing on "a different number of driving steps". Given an interpreter (an operation semantics of a programming language), the interpreter has a concept of a step repeated during evaluation (computation) of the programs on their given/fixed input data. The driving is a meta-extension of the step over the parameterized input data. Unfortunately, the data sets of Prolog and relation programming languages themselves include parameters named "free variables". That leads to a confusion.

Page 9.
31) Typo:
  Figure 3: Execution time of evalo --> Figure 3: Execution time of eval$^o$

Sec. 4.1.4 The Order of Answers
Page 10.

32) We believe that, in general, it is not possible to guarantee the same order of answers after specialization.
 --- That is evidently since, in general, the problem discussed is undecidable.

Sec. 4.2 Typechecker-Term Generator

Page 11.
33) For example, typechecking of the sum of two terms in the hand-written implementation consists of a single conjunction (see Listing 5) while the generated program is far more complicated and also uses a special relation typeEq$^o$ to compare types (see Listing 6).
 --- Here the residual programs should be specialized once again. Is able your model specializer discussed to remove such redundant relation definitions from the residual programs?

Sec. 5 Conclusion
Page 12.
34) We compared this approach with the most sophisticated implementation of conjunctive partial deduction- ECCE partial deduction system-on 6 relations which solve 2 different problems.
 --- Do your really "believe" that such a benchmark size is quite convincing? Don't you?

Thanks for your enthusiasm.

\subsection*{Review 5}

Specialization is a transformation of a given program with respect to (a set of) given restrictions to its behavior. Relational programming offers the ability to run a program in various directions and to execute goals with free variables. The paper describes a novel approach to relational programs specialization. It aims at improving the conjunctive partial deduction (CPD) approach such that it accelerates the generated programs.

In CPD, the specialization is done at the local level, where the shape of the residual program is determined, and at the global level, where every relation in the residual program is defined. At the local level, CPD considers atoms one-by-one from left to right. The paper proposes heuristics to relax the order of atom consideration and to decide if unfolding a relation call can lead to the discovery of contradictions between conjuncts. This leads to the restriction of the answer set at the specialization run.

The main algorithm is described in pseudocode (Alg.1), but it is not easy to follow for readers without a proper background. Below are some suggestions to improve the presentation:
 - residualize -- is not explained.
 - I feel the combination of functional notation and set-theoretic notation is confusing. For instance, line 1 suggests that drive is a function, but in line 2 drive is defined as disjunction drive_disj $\cup$ drive_conj. Then in line 5, drive_disj is defined using drive_conj. So, if I understand correctly, drive can be replaced just by drive_disj, and line 2 of the algorithm can be completely removed.
 - What do the C@ and D@ prefixes denote?
 - The presentation of the main idea of joining instead of splitting is somewhat dry. The motivation is more-or-less understandable, but the technical description is hard to follow. An example would greatly help here.

The paper also presents a heuristic to control the unfolding by computing if the unfolded relation contains fewer disjuncts than it could possibly have. Still, I suggest the authors polish the technical sections and add a running example.

The related work section actually presents both the related work and background. For readability reasons, I suggest separating it into two sections; as well as introducing the notation and giving more background on MINIKANREN.

My main criticism is about the evaluation. There are two case studies: 1) a subset of propositional formulas and 2) type checking for a simple language. The approach is implemented in OCANREN (statically typed MINIKANREN embedding in OCAML) and compared to ECCE (the most mature implementation of CPD for PROLOG). The implementation of the approach (called ConsPD) outperforms ECCE almost always. But the running times in both cases are very short (seconds, or fractions of seconds). An experiment over larger benchmarks would look more convincing. Compare to SAT solvers -- they are pretty good at solving formulas with thousands and millions of variables. The task considered in this paper is way simpler because no actual solving is done. I then suggest generating some really large propositional formulas and copmare ECCE and ConsPD on them.

To sum up, I think the paper gives an interesting contribution, but the presentation and experiments could be improved.
