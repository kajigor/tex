\section{Related Work}

\subsection{Logic Programming Languages}

Multiple logic programming languages have been developed over the years with \prolog~\cite{battani1973interpreteur} being the most widespread among them. 
It was the first successful attempt to enable declarative programming by means of writing programs in a subset of formal logic.
At its core, it uses Horn clauses, a decidable subset of first-order predicate logic. 
Each program formulates a set of facts and predicates that connect the facts together. 
The evaluation of a program is done by an SLD resolution~\cite{robinson1965machine} of a query, and often follows depth-first approach.  

For years, logic programming was highly limited by hardware capabilities, which lead to compromises being made. 
One of them was an early removal of occurs-check from the unification algorithm~\cite{cohen1988view}. 
This means that running a query \texttt{? f(X, a(X)).}, given a program \texttt{f(X, X).}, produces a nonsensical result \texttt{X $\mapsto$ a(X)}. 
It is up to the user to ensure that a variable never occurs in a term it is unified with. 
Fortunatelly, the user can use a special sound unification predicate such as \texttt{unify\_with\_occurs\_check} to prevent such results. 

The other compromise is linked to the implementation details and has more dire consequences. 
Logic languages are inherently nondeterministic, and evaluation on a deterministic computer warrants decisions about how to explore the search space. 
Majority of \prolog implementations feature depth-first search, which often results in either non-termination or the generation of infinitely many similar answers to a query. 
To combat this drawback, non-relational constructs such as cuts and copy-terms have been adopted. 
Unfortunately using them often limits a relation to a single mode and directly contradicts the main idea of declarative programming itself: a program can no longer be written with disregard of the pecularities of the language. 

Recently, there have been a resurgence of the logic programming paradigm as more  languages have emerged, including \merc\footnote{The website of the \merc programming language \url{https://mercurylang.org/}}, \curry\footnote{The website of the \curry programming language \url{https://curry.pages.ps.informatik.uni-kiel.de/curry-lang.org/}}, \mk\footnote{The website of the \mk programming language \url{http://minikanren.org/}}, and others.
Morever, a prominent video games developer Epic Games invested into designing a new functional-logic programming language~\cite{versecalculus}.
This new generation of logic languages mix the paradigms of logic and, more mainstream, fuctional programming. 
\merc and \curry are stand-alone logic-functional programming languages with dedicated compilers that makes it difficult to interoperate with bigger systems typically written in a general-purpose language. 

On the other hand, \mk is implemented as a lightweight embedded domain specific language.
It makes it possible to bring the power of logic programming in any general purpose language. 
\mk features interlieving search~\cite{kiselyov2005backtracking} that guarantees that every solution to a query will be found, given enough time. 
Moreover, its extendible architecture allows for easy experimentation and addition of new features. 
The main design philosophy of \mk is to adhere to the pure logic programming as much as possible, so that any program could be called in any given direction. 
Taking into the account all these considerations, we chose \mk as the main language in this research. 

\subsection{Specialization}

The first specialization method called supercompilation was introduced by Turchin in 1986~\cite{turchin1986concept}.
It was designed for the Refal programming language~\cite{turchin1989refal} that was significantly different from the mainstream languages of the time.
Since then, it was formulated for different languages, expanding its utility across various programming paradigms~\cite{klyuchnikov2009supercompiler,mitchell2010rethinking}.
Apart from it, many modifications appeared that feature alternative termination strategies, generalization and splitting techniques~\cite{leuschel2002homeomorphic,sorensen1995algorithm,turchin1988algorithm}. 

Partial evaluation first developed by Jones~\cite{jones1993partial,intro2partialEvaluation} is an optimization technique that is capable of using information about program arguments.
When some of the arguments is known, partial evaluator precomputes parts of the program dependent on them and generates a more efficient program. 

In the area of logic programming, a general framework called rules + strategies, or fold/unfold transformations, was introduced by Pettorossi and Proietti~\cite{pettorossi1996rules,pettorossi1994transformation}. 
It serves as a foundational theory for many semantics-preserving transformations, including tupling, specialization, compiling control, and partial deduction. 
Unfortunately, this work depend on the user to guide the control decisions, its termination is not always guaranteed, and because of it its autormation is complicated. 

Specialization in logic programming is commonly refered to as partial deduction. 
It was introduced by Komorowski~\cite{komorowski1982partial} and formalized by Lloyd and Shepherdson~\cite{lloyd1991partial}. 
Comparing to the fold/unfold transformations, partial deduction is less powerful, because it considers every atom on its own and does not track the dependecies between variables. 
At the same time, it is significantly easier to control and can be autumated. 

The main drawback of partial deduction is addressed by Leuschel with conjunctive partial deduction~\cite{de1999conjunctive} in the ECCE system. 
It makes use of the interaction between conjuncts for specialization, getting rid of some repeating traversals of data structures as a result. 
We implemented this algorithm as a proof-of-concept for \texttt{miniKanren}, and found out that some of the specialization results were subpar.
In some cases, the specialized programs had worse performance than the original ones. 

Depending on whether control decisions are taken before or during specialization stage, partial evaluators are separated into offline and online methods. 
LOGEN is the implementation of the offline approach for logic programming developed by Leuschel~\cite{leuschel2004offline}. 
It includes an automatic binding-time analysis to derive annotations used to guide the specialization process. 
Offline specialization usually takes less time than online, and is capable to generate shorter and more efficient programs. 

The fact that majority of \prolog implementations do not impose a type system may be seen as a disadvantage when it comes to optimizations. 
\merc developed a strong static type and mode system that can be used in compilation~\cite{overton2002constraint,overton2003precise}. 
Mode analysis embodies data-flow analysis that makes it possible to compile the same definition into several functions specialized for the given direction. 
